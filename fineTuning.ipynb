{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GMA9UtSm4-C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOvTJp7RnZBd"
   },
   "outputs": [],
   "source": [
    "csv_path = \"../data/final_texts.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data set into 80% training, 10% test and 10% validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qBZucn8_n9cV"
   },
   "outputs": [],
   "source": [
    "train_texts, temp_texts, train_summaries, temp_summaries = train_test_split(\n",
    "    df[\"text\"], df[\"summary\"], test_size=0.2, random_state=42\n",
    ")\n",
    "val_texts, test_texts, val_summaries, test_summaries = train_test_split(\n",
    "    temp_texts, temp_summaries, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CxRoxBMLtpWL"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\"text\": train_texts, \"summary\": train_summaries})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_texts, \"summary\": val_summaries})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts, \"summary\": test_summaries})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fine-tune **Google t5-base**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QL4agTwgoFsQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 768)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google-t5/t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration for Fine-Tuning T5\n",
    "\n",
    "We'll fine-tune a T5 model using LoRA (Low-Rank Adaptation). LoRA is a parameter-efficient fine-tuning method that significantly reduces the number of trainable parameters while maintaining performance.\n",
    "\n",
    "### What is LoRA?\n",
    "\n",
    "LoRA works by adding low-rank matrices to the original model weights instead of fine-tuning all parameters. This approach:\n",
    "- Reduces memory usage\n",
    "- Accelerates training\n",
    "- Produces smaller fine-tuned models\n",
    "\n",
    "### Our Configuration\n",
    "\n",
    "We're using the following LoRA configuration:\n",
    "- Rank (r): 8\n",
    "- Alpha: 32\n",
    "- Dropout: 0.1\n",
    "- Target modules: Query (q), Key (k), Value (v), and Output (o) matrices in attention layers\n",
    "\n",
    "This configuration allows us to reduce the number of trainable parameters from 225 million to only 1.8 million (0.79% of the original model), achieving a reduction factor of 127x.\n",
    "\n",
    "### Benefits for Summarization Task\n",
    "\n",
    "By using LoRA for our summarization task:\n",
    "1. We can efficiently fine-tune the T5 model on our dataset\n",
    "2. The resulting model maintains good performance with minimal parameter changes\n",
    "3. The fine-tuned model has a much smaller footprint\n",
    "4. We can adapt the pre-trained model to our specific text summarization needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juWfrY2up2HC"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(r=8, lora_alpha=32, lora_dropout=0.1, target_modules=[\"q\", \"k\", \"v\", \"o\"])\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 224,673,024\n",
      "Trainable parameters: 1,769,472\n",
      "Percentage of parameters being trained: 0.79%\n",
      "Parameter reduction factor: 126.97x\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Percentage of parameters being trained: {100 * trainable_params / total_params:.2f}%\")\n",
    "print(f\"Parameter reduction factor: {total_params / trainable_params:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWSc3ErpqNk0"
   },
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    inputs = tokenizer(\n",
    "        [\"summarize: \" + text for text in data[\"text\"]],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            data[\"summary\"], \n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    inputs[\"labels\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in labels]\n",
    "        for labels in inputs[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "3e35dd14cebf42d89a59cc9a967f3b8b",
      "72040cb737e24ff2bfac8d15e17d363a",
      "a8c0dc96c9d646da94eba248dddf8158",
      "f35d4991d0244b4a87af5d0f4a88b593",
      "01d25e61b1d6408aaefa497dd020adc6",
      "d8219c03d0fb413fb6483507d3960657",
      "8f80dfedbd8a49ac8b8d7e41bc15d6da",
      "c9586d64e02a401c88fb346f938edf8d",
      "1aa029463418458ca6f59729f12fb263",
      "516b53fc0b4d49bb9e7b29b0cf1480a8",
      "a9875e3fb7b643f79970250427486733",
      "7451fa58053d44e7b1fd3a595ae8021c",
      "c437dc21105540b4aa0e4d004640c5e0",
      "8a9748e35968452c840039e8f340976c",
      "18c91e7a81c6458586b175974609fb31",
      "2f3f44e37fe0428ba525f6c78d63e828",
      "24300b98d01a4da7a818c53ac9eb6c31",
      "459e1aa4ce4a41a38c1616354ada1782",
      "9c58675827b348a28525067cd2409e9e",
      "fd2fceca28084080a5642973ea3b6f4e",
      "29e1ad4dd36645f9ac04b6e390359faa",
      "ddf67651874847cdb8f33a760079a7d0",
      "ea22dd90ea6843aa9094ea435ded410d",
      "a3990921592443ada5e10d69c6db8348",
      "b195d8e1576f47d39aff3e5f6a4d055f",
      "02e1ba9a7a414ff0b99dbf923fa8980c",
      "5be026f7a1364bedba4a2864dfdcb9a8",
      "ea6d84773e4b440c81b607e68043cb58",
      "29f247daa54842e19f527ceaa0dce5b0",
      "f568cda1c76f4ee9a5bb6939c89032dd",
      "752dc3f96d684d4fb8082644150a23d5",
      "af8b14ff95324783af1744c39660c919",
      "98fb4d5933b84039a2143f673d60fe55"
     ]
    },
    "id": "OH3-PovPrQUy",
    "outputId": "f98a6fa0-880e-4869-9fad-b9c10fdd10ba"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(preprocessing, batched=True)\n",
    "val_dataset = val_dataset.map(preprocessing, batched=True)\n",
    "test_dataset = test_dataset.map(preprocessing, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AUupmphsD-e"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_summarization\",\n",
    "    report_to=\"none\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    label_names=[\"labels\"],\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "TbCgSM1buCi1",
    "outputId": "010a1357-8a8f-40f7-8957-d127c0557196"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40000' max='40000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40000/40000 1:44:51, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.516400</td>\n",
       "      <td>1.328949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.502200</td>\n",
       "      <td>1.296803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.367800</td>\n",
       "      <td>1.279965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.444000</td>\n",
       "      <td>1.266407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.336000</td>\n",
       "      <td>1.264849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.457900</td>\n",
       "      <td>1.256821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.434000</td>\n",
       "      <td>1.249727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.307900</td>\n",
       "      <td>1.242358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.337100</td>\n",
       "      <td>1.239278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.425900</td>\n",
       "      <td>1.236546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.321300</td>\n",
       "      <td>1.236594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.268100</td>\n",
       "      <td>1.232432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.165100</td>\n",
       "      <td>1.229083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.292300</td>\n",
       "      <td>1.229892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.321700</td>\n",
       "      <td>1.229391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.371600</td>\n",
       "      <td>1.226462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.327300</td>\n",
       "      <td>1.226144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.293100</td>\n",
       "      <td>1.227049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.262400</td>\n",
       "      <td>1.226674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.351300</td>\n",
       "      <td>1.226426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=40000, training_loss=1.3753213768959045, metrics={'train_runtime': 6293.0592, 'train_samples_per_second': 12.712, 'train_steps_per_second': 6.356, 'total_flos': 4.915149668352e+16, 'train_loss': 1.3753213768959045, 'epoch': 20.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "gVLzvJvtuJUf",
    "outputId": "d8800410-24a8-4d35-8b58-85ebafbde326"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3135524988174438,\n",
       " 'eval_runtime': 12.8454,\n",
       " 'eval_samples_per_second': 38.924,\n",
       " 'eval_steps_per_second': 19.462,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wm4nt-JYuR9K",
    "outputId": "5b0c2103-d569-4209-b213-93e875f66362"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./lora_summarization/tokenizer_config.json',\n",
       " './lora_summarization/special_tokens_map.json',\n",
       " './lora_summarization/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./lora_summarization\")\n",
    "tokenizer.save_pretrained(\"./lora_summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "F4jLY7Tc7SBb"
   },
   "outputs": [],
   "source": [
    "def generate_summary(batch,model):\n",
    "    inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=\"longest\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    summary_ids = model.generate(**inputs, max_length=128, num_beams=5)\n",
    "    batch[\"generated_summary\"] = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YNRm60M6_htL"
   },
   "outputs": [],
   "source": [
    "model_name_base = \"google-t5/t5-base\"\n",
    "model_base = AutoModelForSeq2SeqLM.from_pretrained(model_name_base)\n",
    "\n",
    "model_name_ft = \"./lora_summarization\"\n",
    "model_ft = AutoModelForSeq2SeqLM.from_pretrained(model_name_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "f002f4427884427b9183b313e487db07",
      "6d97e07553b746188d7ba906b2e81b14",
      "abc4713a545745c8b7432a081916716c",
      "71e3952a5f9d46dd8752851b9eda9ff9",
      "500919c879dd456c93ffd68018cc5407",
      "978be874d8bc49e2a29d33dc2483e750",
      "e40d3b51cd0d45a58de983592c7f6bab",
      "07a60a92fdc64a7e98e6e7debd14e815",
      "ce520df3f6dd4aeaa00589f78ac68772",
      "b2c80fd669d24d29869b745088cfe199",
      "ca964e44de5c4b2d9009e28b463e2d6d",
      "581006e0df9a44c998f0810617e3ab41",
      "a3d3c148d8034abd8b47f8e49c1cc5a9",
      "8e537ac6f9244f868a74ec9d03f5315b",
      "972bed56564b41d7883cc8ef72d71913",
      "aa1595e7bd7b47d1850731acdfc0c1d3",
      "ae709cc71d854a9ea353533a87a4b2e2",
      "bebe22fb16204628a484486d458f5979",
      "abfb496873b24a9ea6a35b58018d5d88",
      "053206cd40514b6394986442c2d60935",
      "4abd76193fc8422f9e5b8242648d39cc",
      "ccac27372e7c487eb811dd473eed7312"
     ]
    },
    "id": "Ei8rBep_AsOy",
    "outputId": "7180c09e-0be3-4407-c0bc-fee4327600ac"
   },
   "outputs": [],
   "source": [
    "val_dataset_scored_base = val_dataset.map(lambda batch : generate_summary(batch, model_base))\n",
    "val_dataset_scored_ft = val_dataset.map(lambda batch : generate_summary(batch, model_ft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_list(example):\n",
    "    example['generated_summary_clean'] = example['generated_summary'][0] if isinstance(example['generated_summary'], list) else example['generated_summary']\n",
    "    return example\n",
    "\n",
    "val_dataset_scored_base = val_dataset_scored_base.map(extract_from_list)\n",
    "val_dataset_scored_ft = val_dataset_scored_ft.map(extract_from_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "We use bert score to compute metrics on our results. The BERTScore metrics reveal improvements after fine-tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7TeYkskYAuTo",
    "outputId": "3cbe0df6-4114-47ea-e1d1-54d09e3ddf2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision base: 0.8328\n",
      "BERTScore Recall base: 0.8294\n",
      "BERTScore F1 base: 0.8309\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision base: 0.8584\n",
      "BERTScore Recall base: 0.8627\n",
      "BERTScore F1 base: 0.8603\n"
     ]
    }
   ],
   "source": [
    "P_base, R_base, F1_base = score(val_dataset_scored_base[\"generated_summary_clean\"], val_dataset_scored_base[\"summary\"], lang=\"fr\" if \"fr\" in model_name else \"en\")\n",
    "\n",
    "print(f\"BERTScore Precision base: {P_base.mean():.4f}\")\n",
    "print(f\"BERTScore Recall base: {R_base.mean():.4f}\")\n",
    "print(f\"BERTScore F1 base: {F1_base.mean():.4f}\")\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "P_ft, R_ft, F1_ft = score(val_dataset_scored_ft[\"generated_summary_clean\"], val_dataset_scored_ft[\"summary\"], lang=\"fr\" if \"fr\" in model_name else \"en\")\n",
    "\n",
    "print(f\"BERTScore Precision base: {P_ft.mean():.4f}\")\n",
    "print(f\"BERTScore Recall base: {R_ft.mean():.4f}\")\n",
    "print(f\"BERTScore F1 base: {F1_ft.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_scored_base.to_csv(\"../data/val_dataset_scored_base.csv\")\n",
    "val_dataset_scored_ft.to_csv(\"../data/val_dataset_scored_ft.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: DU MÊME AUTEUR ROMAU L~ Fus DB CoRAUE, a~' édition, t vol. in-t8 3 5o Ls MARtAOE c'OoBTTE, ta édition, t vol. ! n-~8 3 5o LE P&RB M MARTfAi., ~e édition, t vol. tn-t8.. 3 5o LA MAttQUtss, ~y édjtton, t vol. in-! 8. 3 50 LES AMOURS otON.Mss, g édition, ï vol. in-t8.. 3 5o SoLANSB Du CMtx-SAMT-Luc, 35~ édition, tvoL ia.t8 3 5o MAt'BMO! 8E!.t.) E BBBtttsstER, 228 édition, t vol. in t8 3 5c Ta~~smE, 3os édMon, t vol. in-t8 3 5o DISPARU, Bae édition, voL ia-t8 3 5o MH ttONB* ONt e'tx WA t\n",
      "Summary: Romain Lefebvre : \"Du Même Auteur\" : \"Fus DB Coraue\", \"Le Péril Martial\", \"La Matquisse\", \"Les Amours Otomanes\", \"Solansb du Cmtx-Samt-Luc\", \"Mabbermo! 8E!.t.) et \"Bbbttster\" ; \"Taesme\" ; \"Disparu\" ; \"Mh tonbon on t\".\n",
      "\n",
      "Romain Lefebvre a écrit \"Du Même Auteur\" comprenant \"Fus DB Coraue\", \"Le Péril Martial\", \"La Matquisse\", \"Les Amours Otomanes\", \"Solansb du Cmtx-Samt-Luc\", \"Mabbermo! 8E!.t)\", \"Bbbttster\", \"Taesme\" et \"Disparu\", ainsi que \"Mh tonbon on t\".\n",
      "Generated summary base: in-t8 3 5o Ls MARTfAi., e édition, t vol. ! n-t8.. 3 5o Ls MARTfAi., e édition, t vol. ! n-t8.. 3 5o Ls MARTfAi., e édition, t vol. ! n-t8.. 3 5o Ls MARTfAi., e\n",
      "Generated summary fine-tuned: in-t8 3 5o Ls MARtAOE c'OoBTTE, ta édition, t vol. n-t8.. 3 5o Ls MARtAOE c'OoBTTE, ta édition, t vol. n-t8.. 3 5o Ls MARtAOE c'OoBTTE, ta édition, t vol. in-t8.. 3 5o L\n",
      "\n",
      "\n",
      "Text: CHAPITRE XIV. 231 léon vainqueur aurait entrepris de conquérir le monde. Mais il n'était pas en son pouvoir de diminuer le froid d'un seul degré, ou même de procurer pour un seul jour de nourriture à ses soldats. Les généraux et les officiers n'étaient plus en état de se faire obéir. La misère avait rompu tous les liens dé là discipline. Les soldats marchaient sans ordre, quittaient leurs drapeaux, s'arrêtaient ou continuaient leur chemin, chacun suivant sa fantaisie.\n",
      "Summary: Léon le vainqueur, incapable de réchauffer le climat ou d'approvisionner ses soldats, perd le contrôle de son armée, qui marche désordonnée, abandonne ses drapeaux et agit à sa guise. La discipline s'est rompue à cause de la misère.\n",
      "Generated summary base: CHAPITRE XIV. 231 léon vainqueur aurait entrepris de conquérir le monde. Mais il n'était pas en son pouvoir de diminuer le froid d'un seul degré, ou de procurer pour un seul jour de nourriture à ses soldats.\n",
      "Generated summary fine-tuned: Le léon vainqueur avait entrepris de conquérir le monde, mais il n'était pas en mesure de réduire le froid d'un seul degré, ni de procurer de nourriture à ses soldats. La misère avait rompu tous les liens dé là discipline. Les soldats marchaient sans ordre, quittaient leurs drapeaux, s'arrêtaient ou continuaient leur chemin, chacun suivant sa fantaisie.\n",
      "\n",
      "\n",
      "Text: JEAN SBOGA'R. ÎS'J — Si j'avois le pacte social à ma disposition,' je n'y changerais rien ; je le déchirerais. ■— Le fruit de l'arbre de la science du bien et du mal, c'est la société. La première fois que l'homme s'est enveloppé d'une ceinture de feuillages, il a revêtu l'esclavage et la mort. — Il y a deux instincts très-opposés dans l'homme simple : l'instinct de conservation pour lui et pour ce qui procède de lui ; l'instinct de destruction pour tout ce qui lui est appris et commandé.\n",
      "Summary: Jean Sbogar : si j'avais le pouvoir, je déchirerais le pacte social ; l'homme est soumis à deux instincts opposés : conservation pour lui et destruction pour ce qu'il apprend. La société est le fruit de l'arbre de la science du bien et du mal ; l'homme a revêtu l'esclavage et la mort la première fois qu'il s'est habillé de feuilles.\n",
      "Generated summary base: ÎS'J — Si j'avois le pacte social à ma disposition,' je ne changerais rien ; je le déchirerais. ÎS'J — La première fois que l'homme est enveloppé d'une ceinture de feuillages, il a revêtu l'esclavage et la mort.\n",
      "Generated summary fine-tuned: JEAN SBOGA'R. ÎS'J — Si j'avois le pacte social à ma disposition, je ne change rien ; je le déchirerais. La première fois que l'homme est enveloppé d'une ceinture de feuillages, il revêt l'esclavage et la mort. Il y a deux instincts très opposés dans l'homme simple : l'instinct de destruction pour tout ce qui lui est appris \n",
      "\n",
      "\n",
      "Text: XIII COMMENT L'URUBUS FIT VISITE A SES PRISONNIERES, ET COMMENT L'OISEAU-DE-NUIT NE FUT PAS DE SON AVIS, ET CE QUI EN ADVINT. Lés pirates étaient enivrés de leur victoire, qu'ils n'avaient pasespérée aussi complète. En effet, ils avaient regagné leur repaire sans être sérieusement inquiétés par les Comanches et les Vaqueros. Ils attribuaient ce résultat à l'enlèvement des trois dames, enlèvement qui avait dû atterrer don Agostin de Sandoval et ses fils et les: empêcher de prendre les mesures nécessaires. pour poursuivre les ravisseurs.\n",
      "Summary: Les pirates sont enivrés de leur victoire complète contre les Comanches et les Vaqueros. Ils attribuent leur succès à l'enlèvement des trois dames, qui a dérangé don Agostin de Sandoval et ses fils et les empêchés de poursuivre les ravisseurs.\n",
      "Generated summary base: XIV COMMENT L'URUBUS FIT VISITE A SES PRISONNIERES, ET CE QUI EN ADVINT. Les pirates étaient enivrés de leur victoire, qu'ils n'avaient pas été sérieusement inquiétés par les Comanches et les Vaqueros.\n",
      "Generated summary fine-tuned: Lés pirates étaient enivrés de leur victoire, qu'ils n'avaient pas été sérieusement inquiétés par les Comanches et les Vaqueros. Ils attribuaient ce résultat à l'enlèvement des trois dames, enlèvement qui avait dû atterrer Don Agostin de Sandoval et ses fils et les empêcher de prendre les mesures nécessaires pour poursuivre les ravisseurs.\n",
      "\n",
      "\n",
      "Text: L'ADROIT GRIMPEUR.99 compagnons de Saloo aAràient reconnu à l'épreuve combien sa connaissance des produits du pays leur était avantageuse; ils le consultaient à chaque difficulté qui se présentait et ils commençaient à le croire capable de mener à bonne fin les entreprises les plus difficiles. Malgré cela, leur étonnement ne fut pas mince lorsqu'il leur affirma qu'il grimperait au sommet du grand durion. Murtagh nia même carrément la possibilité de la Ghose. « L'ami Saloo se moque de nous, dit-il; c'est tout Ce qu'un écureuil pourrait faire que dé monter à ce tronc si haut; aA7ec cela pas plus d'aspérités que sur le flanc d'un navire doublé de cuivre; rien pour s'accrocher après, il n'y a pas moyen.\n",
      "Summary: Les compagnons de Saloo reconnaissent l'utilité de l'adroit grimpeur. Ils sont étonnés quand il prétend grimper au sommet du grand durion, considérant qu'il n'y a pas assez d'aspérités pour s'y accrocher. Murtagh les rassure en affirmant qu'il n'est qu'une blague de Saloo.\n",
      "Generated summary base: L'ADROIT GRIMPEUR.99 les compagnons de Saloo aAràient reconnu à l'épreuve combien sa connaissance des produits du pays leur était avantageuse; ils les consultaient à chaque difficulté qui se présentait et ils les croyaient capables de mener à bonne fin les entreprises les plus difficiles.\n",
      "Generated summary fine-tuned: Murtagh. Les compagnons de Saloo ont reconnu à l'épreuve qu'ils avaient une connaissance avantageuse des produits du pays et qu'ils étaient capables de mener à bonne fin les entreprises les plus difficiles. Murtagh a affirmé qu'il n'y avait pas plus d'aspérités sur le flanc d'un navire doublé de cuivre. Il n'y avait pas moyen.\n",
      "\n",
      "\n",
      "Text: Soudain, l'idée lui passa par la tête qu'il ne trouverait pas de joujoux chez la tante Antoinette, qui était vieille et n'avait pas d'enfants, et il courut chercher les siens. Bientôt, le plancher en fut couvert tout autour de Marie, malgré ses énergiques protestations contre cette invasion de polichinelles, fusils, sabres, canons et locomotives, au milieu des chaussettes et des chemises qu'elle empilait dans la malle. — Assez, monsieur Jean, assez! Vous n'aurez que faire de tout cela, làbas; vous trouverez bien d'autres amusements.\n",
      "Summary: Jean se doute qu'il ne trouvera pas de jouets chez sa tante Antoinette. Il court chercher les siens. La pièce est rapidement envahie par ses jouets, malgré les protestations de Marie, qui empile des chaussettes et des chemises. Jean doit se contenter de ses jouets, car il ne trouvera pas d'autres amusements là-bas.\n",
      "Generated summary base: Bientôt, le plancher en fut couvert toutautour de Marie, malgré ses énergiques protestations contre cette invasion de polichinelles, fusils, sabres, canons et locomotives, au milieu des chaussettes et des chemises qu'elle empilait dans la malle.\n",
      "Generated summary fine-tuned: Soudain, le plancher en fut couvert tout autour de Marie, malgré ses protestations énergiques contre cette invasion de polichinelles, fusils, sabres, canons et locomotives, au milieu des chaussettes et des chemises qu'elle empilait dans la malle.\n",
      "\n",
      "\n",
      "Text: Quand elle eut fini, la mère essuya ses larmes du revers de sa main. — Est d' ma faute, murmura-t-elle, j'aurais point dû rester à la maison l' lundi d' Pâques. J'aurais point dû t'laisser aller aux vêpres, l' dimanche qu' la vache a fait son veau. Est de ma faute... Un attendrissement gagnait Zéfine, devant l'immense bonté de sa mère, qu'elle découvrait seulement maintenant, de même qu'elle mesurait seulement à cette heure toute sa tendresse pour elle.\n",
      "Summary: La mère regrette de ne pas être restée à la maison le lundi de Pâques et de n'avoir pas laissé Zéfine aller aux vêpres. Elle s'excuse et Zéfine découvre son immense bonté.\n",
      "Generated summary base: — Est de ma faute, murmura-t-elle, j'aurais point dû rester à la maison le lundi d'Pâques. J'aurais point dû tisser ses larmes du revers de sa main. — Est de ma faute, murmura-t-elle, j'aurais point dû rester à la maison le lundi d'Pâques. — Est de ma faute, mur\n",
      "Generated summary fine-tuned: Zéfine, devant l'immense bonté de sa mère, découvre qu'elle mesurait à cette heure toute sa tendresse.\n",
      "\n",
      "\n",
      "Text: CHAPITRE PREMIER LE CANDIDAT MALHEUREUX E N l'an 1892, dans la grande cour carrée entourée d'arcades surmontées de longues fenêtres par travées identiques, les élèves du lycée bordant le Boul'Mich, ce jour-là, tournaient en rond, par groupes isolés. Pas de jeux à cette heure, pendant cet après-midi. Seulement des conciliabules, des colloques à voix basse, avec des mines graves. Une sorte d'attente fiévreuse. L'heure est décisive en effet. Les élèves ont passé leurs examens en vue de l'entrée à Saint-Cyr.\n",
      "Summary: Dans la cour carrée du lycée bordant le Boul'Mich en 1892, les élèves tournent en rond en attendant les résultats de leurs examens d'entrée à Saint-Cyr. Il n'y a pas de jeux, seulement des discussions graves. L'heure est décisive.\n",
      "Generated summary base: CHAPITRE PREMIER LE CANDIDAT MALHEUREUX E N l'an 1892, dans la grande cour carrée, entourée d'arcades surmontées de longues fenêtres par travées identiques, les élèves du lycée bordant le Boul'Mich, tournaient en rond, par groupes isolés. Pas de jeux à cette heure, pendant cet après-midi.\n",
      "Generated summary fine-tuned: Les élèves du lycée bordant le Boul'Mich, dans la grande cour carrée entourée d'arcades surmontées de longues fenêtres par travées identiques, tournaient en rond, par groupes isolés. Pas de jeux à cette heure, pendant cet après-midi. Les élèves ont passé leurs examens en vue de l'entrée à Saint-Cyr.\n",
      "\n",
      "\n",
      "Text: L'ESCLAVE, ... 51 rire:; mais ce sourire même glaçait le coeur. Son front avait été enveloppé d'une toile de lin, à travers laquelle suintait; un sang noirci,;, ses paupières, gonfîées par la douleur ,ne pouvaient pluss'ouvrir, et son haleine sortait avec; un sifflement; funeste de ses lèvres déjà blanchies. Arvins, abîmé dans son désespoir, retenait ses.sanglots de peur d'ajouter aux souffrances de sa mère ; mais les quelques: heures qui venaientde s'écouler avaient sillonnée son visage de traces aussi profondes qu'une longue maladie.\n",
      "Summary: L'esclave souriait mais son cœur était glacé. Sa tête était couverte d'une toile de lin qui révélait un visage noirci par la douleur, ses yeux gonflés et ses lèvres blanchies. Arvins, abasourdi par son désespoir, retenait ses sanglots pour ne pas ajouter aux souffrances de sa mère. Son visage portait des traces profondes comme celles d'une longue maladie.\n",
      "Generated summary base: Arvins, abîmé dans son désespoir, retenait ses sanglots de peur d'ajouter aux souffrances de sa mère ; mais les quelques heures qui venaient de s'écouler avaient sillonnée son visage de traces aussi profondes que une longue maladie.\n",
      "Generated summary fine-tuned: Arvins, abîmé dans son désespoir, retenait ses sanglots de peur d'ajouter aux souffrances de sa mère. Les quelques heures qui venaient de s'écouler avaient sillonné son visage de traces aussi profondes qu'une longue maladie.\n",
      "\n",
      "\n",
      "Text: C'est ainsi que Klack, adopté par la compagnie, partagea la vie mouvementée de nos Alpins: il allait partout, sans souci du danger, aimé de tous, tant il était aimable et fidèle. Mais ce n'était pas ainsi que Klack pensait servir la France ! Aussi, au bout de peu de temps, devint-il triste et songeur. A quoi penses-tu, ma vieille? lui faisait familièrement le capitaine, alors qu'un soir Klack était allé se placer près de lui dans la cagna ; on dirait que tu t'ennuies ?\n",
      "Summary: Klack, un chien adopté par les Alpins, partage leur vie mouvementée. Cependant, il n'imaginait pas servir la France de cette manière et devint triste. Le capitaine le questionna un soir dans la cagna. Klack semblait ennuyé.\n",
      "Generated summary base: c'est ainsi que Klack, adopté par la compagnie, partagea la vie mouvementée de nos Alpins : il allait partout, sans souci du danger, aimé de tous, tant il était aimable et fidèle.\n",
      "Generated summary fine-tuned: Klack, adopté par la compagnie, partagea la vie de nos Alpins : il allait partout, sans danger, aimé de tous, tant qu'il était aimable et fidèle. Au bout de peu de temps, Klack devint triste et songeur. On dirait qu'il était ennuié.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Text:\", val_dataset_scored_base[\"text\"][i])\n",
    "    print(\"Summary:\", val_dataset_scored_base[\"summary\"][i])\n",
    "    print(\"Generated summary base:\", val_dataset_scored_base[\"generated_summary_clean\"][i])\n",
    "    print(\"Generated summary fine-tuned:\", val_dataset_scored_ft[\"generated_summary_clean\"][i])\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
